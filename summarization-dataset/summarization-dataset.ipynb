{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"stjiris/t5-portuguese-legal-summarization\"\n",
    "t5_model = AutoModelWithLMHead.from_pretrained(model_checkpoint)\n",
    "\n",
    "'''\n",
    "Em caso de erro no carregamento do tokenizer abaixo, faça a instalação do transformers e sentencepiece convergentes e reinicie o ambiente de execução.\n",
    "\n",
    "pip install datsets transformers[sentencepiece]\n",
    "pip install sentencepiece\n",
    "'''\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# cuda device t5_model\n",
    "t5_model.to('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summarization_jurisprudencia(jurisprudencia):\n",
    "    jurisprudencia = jurisprudencia.replace(\"&nbsp;\", \"\")\n",
    "    inputs = t5_tokenizer.encode(\"summarize: \" + jurisprudencia,\n",
    "                    return_tensors='pt',\n",
    "                    max_length=1000,\n",
    "                    truncation=True).to('cuda:0')\n",
    "    summary_ids = t5_model.generate(inputs, max_length=400, min_length=100,\n",
    "        length_penalty=5., num_beams=2)\n",
    "\n",
    "    output = t5_tokenizer.decode(summary_ids[0])\n",
    "    return output.replace(\"summarize: \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arquivo abaixo gerado em https://github.com/jonas-elias/jurisprudencia-sc-scraping\n",
    "file = open(file='./export_dataset.json')\n",
    "\n",
    "file_content = json.load(file)\n",
    "texto_resumido = []\n",
    "texto_temporario = []\n",
    "\n",
    "# a cada 'x' id`s, um novo arquivo é gerado visando evitar problemas com sessões de processamento (armazenamento gradual)\n",
    "quantidade_arquivo_id = 100\n",
    "\n",
    "for content in file_content:\n",
    "    texto_temporario.append({'id': content['id'], 'texto': content['texto']})\n",
    "    if content['id'] % quantidade_arquivo_id == 0:\n",
    "        if os.path.exists('./summarization-dataset_' + str(content['id']) + '.json') == False:\n",
    "            for texto in texto_temporario:\n",
    "                output = generate_summarization_jurisprudencia(texto['texto'])\n",
    "                texto_resumido.append({'id': texto['id'], 'texto': output})\n",
    "            with open('./summarization-dataset_' + str(content['id']) + '.json', 'w') as f:\n",
    "                json.dump(texto_resumido, f)\n",
    "        texto_resumido = []\n",
    "        texto_temporario = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
